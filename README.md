**Original transformer architecture as per the "Attention is all you need" paper**

This repo contains an implemention from scratch fro the orignal transformer architecture.
To facilitate better understanding, detailed explanations are provided for how the tensor sizes change as the data moves through various layers
