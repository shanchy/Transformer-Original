**Original transformer architecture as per the "Attention is all you need" paper**

This repo contains an implemention from scratch for the orignal transformer architecture.
To facilitate better understanding, detailed explanations are provided within each function for how the tensor sizes change as the data moves through various layers

Here is a illustration of the marvellous transformer architecture:

![TransformerArch](https://github.com/shanchy/Transformer-Original/blob/master/images/transformers_zoomed.png?raw=true)

Understanding the transformer architecture boils down to understanding these five essential components and how they are related to each other.
- The multi-head self-attention block
- The Positional encoding
- The position-wise feed forward network
- The encoder block
- The decoder block
