**Original transformer architecture as per the "Attention is all you need" paper**

This repo contains an implemention from scratch for the orignal transformer architecture.
To facilitate better understanding, detailed explanations are provided for how the tensor sizes change as the data moves through various layers
