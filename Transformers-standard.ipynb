{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original transformer architecture as per the \"Attention is all you need\" paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model,num_heads):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model : dimension of the embedding vectors\n",
    "            n_heads : number of self attention heads\n",
    "            \n",
    "        Return: N/A\n",
    "                    \n",
    "            \n",
    "        Explanation:\n",
    "            Why nn.Linear(d_model,d_model)? If d_model=512, that means a giant 512x512 weight matrix.\n",
    "            If d_model=512, num_heads = 8, then d_head = 64. Shouldn't it be nn.Linear(d_head,d_model)?\n",
    "            Then for an input I = 1x512, and weight matrix W = 64x512, doing I x W.transpose will give 1x64\n",
    "            \n",
    "            Reason is, I=1x512 with W=512x512 will give 1x512. This will be split into 8 parts,1 per head\n",
    "            That would mean the vector fed to each head would be 1x64.\n",
    "            So the computation is done one shot for efficiency, instead of multiply with 8 different matrices\n",
    "            of size 64x512. \n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads # Dimension of the Key, Query & Value vector passed to each head\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "    \n",
    "    def scaled_dot_product_attention (self,Q,K,V,mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Q,K,V : Query, Key & Value matrices. Dimension is (batch_size x num_heads x seq_length x d_head)\n",
    "            mask : masking locations for the decoder\n",
    "            \n",
    "        Return:\n",
    "            Z : Context vectors, from multiple self-attention blocks\n",
    "                Dimenesion is (batch_size x num_heads x seq_length x d_head)\n",
    "        \"\"\"\n",
    "        \n",
    "        #Compute attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(self.d_head)\n",
    "        \n",
    "        #Apply mask if available\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask==0, -1e9)\n",
    "            \n",
    "        #Compute softmax, row-wise, i.e. for a given row all column values are fed to the softmax\n",
    "        attn_probs = torch.softmax(attn_scores,dim=-1)\n",
    "        \n",
    "        #Compute output as summation of weighted V vectors\n",
    "        Z = torch.matmul(attn_probs,V)\n",
    "        return Z\n",
    "    \n",
    "    def split_heads(self,x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x : A tensor representing either Q,K or V, obtained after applying the corresponding W_q, W_k or\n",
    "                W_v to the input batch. Size is (batch_size x seq_length x d_model)\n",
    "        Return: A reordered tensor of size (batch_size x num_heads x seq_length x d_head), which was originally\n",
    "                (batc_size x seq_length x num_heads x d_head). This was resized from the input.\n",
    "                num_heads x d_head = d_model\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view (batch_size, seq_length, self.num_heads, self.d_head).transpose(1,2)\n",
    "    \n",
    "    def combine_heads(self,x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x : A tensor representing the computed context vectors \n",
    "                Dimensions are (batch_size x num_heads x seq_length x d_head)\n",
    "        Return: This function does the reverse operation of split_heads. Therefore, it takes the input and \n",
    "                creates (batch_size x seq_length x d_model)\n",
    "        \"\"\"\n",
    "        batch_size, _, seq_length,d_head = x.size()\n",
    "        return x.transpose(1,2).contiguous().view(batch_size,seq_length,self.d_model)\n",
    "    \n",
    "    \n",
    "    def forward(self,Q,K,V, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Q,K,V : All there parameters get a copy of the input batch as an argument.\n",
    "                    Dimension of input batch is (batch_size x seq_length x d_model)\n",
    "            mask : masking locations - padding if encoder, and padding+look-ahead if decoder\n",
    "        Return:\n",
    "            output : A tensor representing the output of the multi-head self-attention block\n",
    "                    Dimension is (batch_size x seq_length x d_model)\n",
    "        \n",
    "        \n",
    "        Explanation:\n",
    "            Input is a batch of samples. \n",
    "            Assume batch_size = 32. Each sample is a sequence of tokens. Assume seq_length = 16. Each token\n",
    "            will be converted to a embedding. Assume embedding size = 512. \n",
    "            Then, after the embedding is generated for a batch of 32 samples and the positional embeddings are\n",
    "            added, each batch has dimensions (32x16x512) \n",
    "            \n",
    "            ========== Compute the Q, K & V vectors ============\n",
    "            \n",
    "            The weight matrices W_q, W_k & W_v are all 512x512 (see reason in __init__ method). \n",
    "            Q, K & V are computed by apply the corresponding weight matrices to a copy of the input batch. \n",
    "            The resultant matrix's dimensions remains same as the input batch at 32x16x512.\n",
    "            \n",
    "            Each matrix is then resized to (32x16x8x64). This means that for each of the 16 tokens, there are \n",
    "            8 vectors, each of them 64-D, which will be fed in parallel to the 8 attention heads.\n",
    "            For efficient computation, each matrix is rearranged to (32x8x16x64). Now, for each of the 8 heads,\n",
    "            there are 16 vectors (corresponding to 16 tokens in each sequence) of 64-D each.\n",
    "            In other words, each of the 8 attention heads will receive a batch of 32 tensors, where each \n",
    "            tensor will consist of 16 vectors, each of dimension 64-D.\n",
    "        \n",
    "            ========== Compute multi-head self-attention context vectors =========\n",
    "            Attention scores are computed by multiplying K = (32x8x16x64) by transpose of Q = (32x8x64x16) \n",
    "            Note that prior to transpose it was (32x8x16x64). Resultant matrix is 32x8x16x16\n",
    "            This score is rescaled by dividing with sqrt(d_head), where d_head = 64. Next softmax is applied.\n",
    "            Dimensions don't change during the rescaling and softmax\n",
    "            Next the matrix is multiplied by the value matrix. So (32x8x16x16) x (32x8x16x64) -> (32x8x16x64).\n",
    "            This multiplication perform 2 actions to compute the final context vector for each token,: \n",
    "            a) it multiplies the V vectors with the computed attention scores, and\n",
    "            b) sums the vectors.\n",
    "            This is done across all 8 heads in parallel, generating 8 context vectors (64-D) per token \n",
    "            \n",
    "            ========== Compute combined output ===========\n",
    "            The (32x8x16x64) is reversed back to (32x16x8x64) which is (batch_size x seq_length x num_heads\n",
    "            x d_head). It is then rearranged so that for each token, the 8 vectors of 64-D dimension are \n",
    "            concatenated to form a 512 vector, resulting in (32x16x512). This is then passed through a liner\n",
    "            layer with weight matrix 512x512. The final tensor is (32x16x512)\n",
    "            \n",
    "        \"\"\"\n",
    "    \n",
    "        # Compute the Q, K & V vectors \n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Compute multi-head self-attention context vectors\n",
    "        attn_ctxt_vecs = self.scaled_dot_product_attention(Q,K,V, mask)\n",
    "        \n",
    "        # Compute combined output\n",
    "        output = self.W_o(self.combine_heads(attn_ctxt_vecs))\n",
    "        \n",
    "        return(output)\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feed-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFF(nn.Module):\n",
    "    def __init__(self,d_model,d_ff):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model : dimension of the multi-head self-attention output vector\n",
    "            d_ff : dimension of the inner Linear layer, usually much smaller than d_model\n",
    "            \n",
    "        Return: N/A\n",
    "        \"\"\"\n",
    "        super(PositionWiseFF,self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model,d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x : output of the multi-head self-attention block. Size is (batch_size x seq_length x d_model) \n",
    "            \n",
    "        Return: Tensor, the same size as the input x\n",
    "        \n",
    "        Explanation:\n",
    "            Position-wise means that every single token representation will be fed to the neural net.\n",
    "            Feed-forward networks usually comprise of two Linear layers. The first one expands the dimension and\n",
    "            the second one reduces it back\n",
    "            \n",
    "            It is important to note that even though the input d_model is exactly the same as the input \n",
    "            embedding dimension, this is not compulsory. Depending on the weight matrices in the attention \n",
    "            block, this d_model dimension could be smaller/larger compared to the input embedding dimension.\n",
    "            Similarly, the second layer reduces back to the same size as the input. This is also not compulsory\n",
    "            For the sake of simplicity, in this architecture, dimension of d_model is used everywhere.\n",
    "        \"\"\"\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model,max_seq_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model : input embedding dimension\n",
    "            max_seq_length : maximum number of tokens in an input sample\n",
    "        \n",
    "        Return : N/A\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        \n",
    "        # A vector representing the index positions of each token in the sequence, reshaped to a column vector\n",
    "        position = torch.arange(0,max_seq_length,dtype=torch.float).unsqueeze(1)\n",
    "   \n",
    "        # A curve that starts at 1 and decays exponentially towards 0, index increases from 0 -> d_model, at \n",
    "        # interval of 2\n",
    "        div_term = torch.exp(torch.arange(0,d_model,2).float() * -(math.log(10000.0)/d_model))\n",
    "        \n",
    "        # Input embedding has dimension d_model.\n",
    "        # To compute the corresponding positional embedding of similar dimension,\n",
    "        # For any given even position i in the embedding, two values are computed\n",
    "        # 1) Value for that position i = Sin of the div_term, based on that particular position i\n",
    "        # 2) Value of the next position i+1 = Cos of the div_term, based on that same position i\n",
    "        # The position refers to the position of each token in the sequence.\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Unsqueezing helps to make the tensor the same size as the input\n",
    "        self.register_buffer('pe',pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x : input embeddings.Size is (batch_size x seq_length x d_model)\n",
    "        \n",
    "        Return: Tensor, the same size as x\n",
    "        \n",
    "        Explanation:\n",
    "        \n",
    "            The original positional encoding formula is as follows:\n",
    "            PE(pos,2i)     = sin(pos/10000^((2*i)/d_model))\n",
    "            PE(pos,2i + 1) = cos(pos/10000^((2*i)/d_model))\n",
    "        \n",
    "            In this code, the following formula is used\n",
    "            PE(pos,2i)     = sin(pos/e^((i*log(10000))/d_model))\n",
    "            PE(pos,2i + 1) = cos(pos/e^((i*log(10000))/d_model))\n",
    "            \n",
    "            So the primary difference is that for the denominator,\n",
    "            e^((i*log(10000))/d_model) is used instead of 10000^((2*i)/d_model).\n",
    "            However, both of them have similar exponential decay behaviour from 1 towards 0 with as\n",
    "            index increase from 0 -> d_model.\n",
    "            \n",
    "            So the overall behaviour is tha the sine will approach 0 and the cos will approach 1.\n",
    "            The pos value increases the frequency and therefore controls how many oscillations occur before\n",
    "            the curves approach 0/1. \n",
    "            \n",
    "            It is this difference in frequency that makes each curve unique and hence gives each token a unique\n",
    "            embedding based on the token position!\n",
    "            \n",
    "            Note:\n",
    "            The div_term can also be computed as follows.\n",
    "            div_term = 1/torch.exp((torch.arange(0, 50, 2).float() * math.log(10000.0))/50)\n",
    "            \n",
    "            *** The return value is adapted from online articles. It seems wrong.\n",
    "                It should be x + self.pe[:x.size(1),:], because the seq_length is at the first dimension\n",
    "                of pe, not the second\n",
    "        \"\"\"\n",
    "        #return x + self.pe[:,x.size(1)]\n",
    "        return x + self.pe[:x.size(1),:]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,d_model,num_heads,d_ff,dropout):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model : dimension of the input samples\n",
    "            num_heads : number of attention heads\n",
    "            d_ff : number of nodes in the hidden layer of the feed forward network\n",
    "            droput : dropout ratio\n",
    "            \n",
    "        Return: N/A\n",
    "        \"\"\"\n",
    "        super(EncoderBlock,self).__init__()\n",
    "        self.mult_attn = MultiHeadAttention(d_model,num_heads)\n",
    "        self.feed_forward = PositionWiseFF(d_model,d_ff)\n",
    "        self.norm_1 = nn.LayerNorm(d_model)\n",
    "        self.norm_2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self,x,mask):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            x : Input to the encoder. Size is usually (batch_size x seq_length x d_model)\n",
    "            mask : specifies which token positions in the sequence are padded\n",
    "\n",
    "        Return:\n",
    "            x : an output tensor, similar in size to the input x\n",
    "            \n",
    "        Explanation:\n",
    "            Each encoder block has 2 main components\n",
    "            a) The multi-head self attention\n",
    "            b) The feed forward network\n",
    "            \n",
    "            First, the multi-head attention is computed for the encoder input. Any necessary padding masks are\n",
    "            specified. Add and norm is performed on the output\n",
    "            This is then fed to the position-wise feed forward network. Add and norm is peformed on the output\n",
    "        \"\"\"\n",
    "        attention_output = self.mult_attn(x,x,x,mask)\n",
    "        x = self.norm_1(x + self.dropout(attnention_output))\n",
    "        feedforward_output = self.feed_forward(x)\n",
    "        x  = self.norm_2(x + self.dropout(feedforward_output))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,d_model,num_heads,d_ff,dropout):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model : dimension of the input samples\n",
    "            num_heads : number of attention heads\n",
    "            d_ff : number of nodes in the hidden layer of the feed forward network\n",
    "            droput : dropout ratio\n",
    "            \n",
    "        Return: N/A\n",
    "        \"\"\"\n",
    "        super(DecoderBlock,self).__init__()\n",
    "        self.mult_attn = MultiHeadAttention(d_model,num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model,num_heads)\n",
    "        self.feed_forward = PositionWiseFF(d_model,d_ff)\n",
    "        self.norm_1 = nn.LayerNorm(d_model)\n",
    "        self.norm_2 = nn.LayerNorm(d_model)\n",
    "        self.norm_3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x,encoder_output, source_mask, target_mask):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            x : Input to the decoder. Size is usually (batch_size x seq_length x d_model)\n",
    "            encoder_output : Output from the final encoder block. Size is (batch_size x seq_length x d_model)\n",
    "            source_mask : specifies which token positions in the input to encoder is padded\n",
    "            target_mask : specifies the look-ahead masks for the input to the decoder \n",
    "\n",
    "        Return:\n",
    "             : an output tensor, similar in size to the input x\n",
    "             \n",
    "        Explanation:\n",
    "            Each decoder block has 3 main components\n",
    "            a) The masked multi-head self attention\n",
    "            b) The normal multi-head attention\n",
    "            c) The position-wise feed forward network\n",
    "            \n",
    "            First, the masked multi-head attention is computed for the decoder input. During training, \n",
    "            the decoder input is the target sequence, but specified with the look-ahead mask. The look-ahead\n",
    "            mask ensures that for each current token in the target sequence, it only has access to information \n",
    "            from preceding tokens. Succeeding tokens are masked and hence cannot contribute to the computations\n",
    "            of the current token.             \n",
    "            Add and norm is performed on the output of the masked multi-head attention.\n",
    "            Next, the output is fed to a cross multi-head attention and used to compute K. \n",
    "            This cross multi-head attention also receives the outputs from the final encoder block, which\n",
    "            are used to compute Q & V.\n",
    "            Add and norm is performed on the output of the cross multi-head attention.\n",
    "            This is then fed to the position-wise feed forward network. Add and norm is peformed on the output\n",
    "        \"\"\"\n",
    "        attention_output = self.mult_attn(x,x,x,target_mask)\n",
    "        x = self.norm_1(x + self.dropout(attnention_output))\n",
    "        attention_output = self.cross_attn(x,encoder_output,encoder_output,source_mask)\n",
    "        x = self.norm_2(x + self.dropout(cross_attention_output))\n",
    "        feedforward_output = self.feed_forward(x)\n",
    "        x  = self.norm_3(x + self.dropout(feedforward_output))\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The transformer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,source_vocabsize,target_vocabsize,\n",
    "                 d_model,num_heads,num_layers,d_ff,max_seq_length,droput):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            source_vocabsize : size of the source vocabulary\n",
    "            target_vocabsize : size of the target vocabulary\n",
    "            d_model : dimension of input embedding\n",
    "            num_heads : number of self-attention heads\n",
    "            num_layers : number of encoder and/or decoder blocks\n",
    "            d_ff : number of nodes in the inner layer of the feed-forward network\n",
    "            max_seq_length: max length of an input/output sequence\n",
    "            droput : dropout ratio\n",
    "\n",
    "        Return: N/A\n",
    "        \n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(source_vocabsize, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(target_vocabsize, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model,max_seq_length)\n",
    "        self.encoder_layers = nn.ModuleList([EncoderBlock(d_model,num_heads,d_ff,droput) \n",
    "                                            for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderBlock(d_model,num_heads,d_ff,droput) \n",
    "                                            for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(d_model,target_vocabsize)\n",
    "    \n",
    "    def generate_mask(self,source,target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            source: Input tensor of size (batch_size x sequence_length). Each row consists of a sequence of \n",
    "                    vocabulary indices, which is used to obtain embeddings. If the number of tokens in the \n",
    "                    sequence is less than \"sequence_length\", the remaining positions have a <pad> token, represented\n",
    "                    by a 0 index\n",
    "                    \n",
    "                    Eg, \n",
    "                    sequence length is 16.\n",
    "                    let sample be the following sentence \"I like cats and dogs, but I prefer dogs\". Length = 10.\n",
    "                    Therefore, the remaining 6 positionos are padded as follows\n",
    "                    I like cats and dogs , but I prefer dogs <pad> <pad> <pad> <pad> <pad> <pad>\n",
    "                    Converted to indices: 32 458 9128 27 9456 5 46 32 7321 9456 0 0 0 0 0 0\n",
    "                    \n",
    "            target: Input tensor of size (batch_size x sequence_length). Details, similar to source\n",
    "\n",
    "\n",
    "        Return:\n",
    "            source_mask: Tensor of boolean values, where padded positions are masked. The tensor will have \n",
    "                         dimensions such that it can be applied to the encoder multi-head attention scores\n",
    "                         Dimension will be (batch_size x 1 x 1 x sequence_length). See Explanation\n",
    "            target_mask: Tensor of boolean values, where padded and look-ahead positions are masked. Tensor\n",
    "                         dimensions must match the masked multi-head attentions scores and the encoder-decoder\n",
    "                         multi-head attention scores.\n",
    "                         Dimension will be (batch_size x 1 x sequence_length x sequence_length). See Explanation\n",
    "                         \n",
    "        Explanation:\n",
    "            Let's assume a batch size of 3, with sequence length of 5. \n",
    "            So input size is (batch_size x sequence_length) = (3 x 5)\n",
    "            \n",
    "            source = [[35, 87, 234, 1233, 0],\n",
    "                      [56, 12, 231, 2323, 722], \n",
    "                      [9121, 444, 0, 0 ,0]]\n",
    "                      \n",
    "            target = [[723, 823, 31, 0, 0],\n",
    "                      [981, 323, 3095, 31, 0],\n",
    "                      [91, 8756, 8123, 231, 88]]\n",
    "                      \n",
    "            =====================================          \n",
    "            First, the source_mask needs to be obtained. The source values are converted to boolean, where False indicates\n",
    "            masked positions, as follows\n",
    "            [[ True,  True,  True,  True, False],    <-- sample 1\n",
    "             [ True,  True,  True,  True,  True],    <-- sample 2\n",
    "             [ True,  True, False, False, False]].   <-- sample 3\n",
    "             \n",
    "             This tensor is the same size as the input, which is (3 x 5). However it needs to be adjusted so that\n",
    "             it can be applied to the attention scores.\n",
    "             Assuming, there are 8 heads, the tensor of attention scores will be (3 x 8 x 5 x 5)\n",
    "             \n",
    "             Performing unsqueeze(1), follwed by unsqueeze(2), converts the (3x5) mask to (3 x 1 x 1 x 5).\n",
    "             Now this can be applied to the attention scores and is returned by the function\n",
    "            \n",
    "             BUT HOW WILL IT BE APPLIED TO THE ATTENTION SCORES???\n",
    "             \n",
    "             Let's look at sample 1 mask = True, True, True, True, False. This means the 5th position is masked\n",
    "             Remember that for each sample, there are 8 heads, and each head has attention scores of 5x5,\n",
    "             and hence the last three dimension of 8x5x5 in (3x8x5x5).\n",
    "             During the mask application, the boolean value is broadcasted across the second dimension to become\n",
    "             \n",
    "             [[True, True, True, True, False],\n",
    "              [True, True, True, True, False],\n",
    "              [True, True, True, True, False],\n",
    "              [True, True, True, True, False],\n",
    "              [True, True, True, True, False]].\n",
    "              \n",
    "              Do notice how the 5th position that was padded always gets masked out.\n",
    "              \n",
    "              This is then broadcasted across the 3rd dimension, so that each of the heads for the 1st sample\n",
    "              gets this same 5x5 tensor as the mask.\n",
    "              \n",
    "              This broadcasting occurs in this manner due to the dimensionality of 1 in (3 x 1 x 1 x 5).\n",
    "              \n",
    "              In similar fashion, the 2nd sample is as follows after broadcasting\n",
    "              \n",
    "              [[ True,  True,  True,  True,  True],\n",
    "               [ True,  True,  True,  True,  True],\n",
    "               [ True,  True,  True,  True,  True],\n",
    "               [ True,  True,  True,  True,  True],\n",
    "               [ True,  True,  True,  True,  True]]\n",
    "              \n",
    "               and the 3rd sample is \n",
    "               [[ True,  True, False, False, False],\n",
    "                [ True,  True, False, False, False],\n",
    "                [ True,  True, False, False, False],\n",
    "                [ True,  True, False, False, False],\n",
    "                [ True,  True, False, False, False]]\n",
    "             \n",
    "                Take not how the 2nd sample has no padding, whereas the 3rd sample always has the 3rd - 5th\n",
    "                positions padded.\n",
    "                \n",
    "                ============================\n",
    "                Next, the target mask needs to obtained.\n",
    "                The target mask goes through the same masking process for padded tokens as the source mask, \n",
    "                but, it also has to go through look-ahead masking.\n",
    "                \n",
    "                When applying masking to the target for the padded tokens, the target_mask is as following.\n",
    "                \n",
    "                [[ True,  True,  True,  False, False],    <-- sample 1\n",
    "                 [ True,  True,  True,  True,  False],    <-- sample 2\n",
    "                 [ True,  True,  True,  True,  True]].    <-- sample 3\n",
    "                 \n",
    "                Similar to the source, this is (3 x 1 x 1 x 5) and will be applied to the attention scores via\n",
    "                broadcasting.\n",
    "                \n",
    "                The look head mask, called the nopeak_mask is based on the sequence length is obtained by\n",
    "                a function that applies masks diagonally. In this case, the sequence length is 5, hence\n",
    "                nopeak_mask is a follows:\n",
    "                \n",
    "                [[ True, False, False, False, False],  \n",
    "                 [ True,  True, False, False, False],\n",
    "                 [ True,  True,  True, False, False],\n",
    "                 [ True,  True,  True,  True, False],\n",
    "                 [ True,  True,  True,  True,  True]]\n",
    "                 \n",
    "                 Notice the diagonal nature. When this mask is applied to the attention score of (5x5),it means\n",
    "                 that at any given position in that sequence, the tokens only have access to information from\n",
    "                 itself and previous tokens, not the tokens after it. During training, the target is actually fed\n",
    "                 to the decoder, and therefore this masking is mandatory because the model should not be able to \n",
    "                 see the tokens that it is going to predict. \n",
    "                 \n",
    "                 The final target mask is obtained by doing target_mask & nopeak_mask\n",
    "                 Note that this nopeak_mask can be (5x5) or (1x5x5). Either way it is compatible with the\n",
    "                 target_mask which is (3x1x1x5) \n",
    "                 \n",
    "                 Do notice that during this & operation, the values in the 1st dimension of the target_mask\n",
    "                 are broadcasted across the second dimension to get (3x1x5x5) to match(1x5x5).\n",
    "                 \n",
    "                 After the & operation, the resultant target mask becomes a (3x1x5x5):\n",
    "                 \n",
    "                 [[[[ True, False, False, False, False],\n",
    "                    [ True,  True, False, False, False],     \n",
    "                    [ True,  True,  True, False, False],     5x5 tensor for sample 1, applied to all 8 heads\n",
    "                    [ True,  True,  True, False, False],     Notice the last 2 positions are always masked,\n",
    "                    [ True,  True,  True, False, False]]],   in addition to the look-ahead mask\n",
    "\n",
    "\n",
    "                  [[[ True, False, False, False, False],\n",
    "                    [ True,  True, False, False, False],\n",
    "                    [ True,  True,  True, False, False],     5x5 tensor for sample 2, applied to all 8 heads\n",
    "                    [ True,  True,  True,  True, False],     Notice the last position is always masked,\n",
    "                    [ True,  True,  True,  True, False]]],   in addition to the look-ahead mask\n",
    "\n",
    "\n",
    "                  [[[ True, False, False, False, False],\n",
    "                    [ True,  True, False, False, False],\n",
    "                    [ True,  True,  True, False, False],    5x5 tensor for sample 3 applied to all 8 heads\n",
    "                    [ True,  True,  True,  True, False],    Notice no position is always masked. \n",
    "                    [ True,  True,  True,  True,  True]]]]  Only the look-ahead mask exists\n",
    "                    \n",
    "                This (3x1x5x5) tensor is the target_mask is returned by the function\n",
    "                     \n",
    "        \"\"\"\n",
    "        source_mask = (source !=0 ).unsqueeze(1).unsqueeze(2)\n",
    "        # target_mask = (target !=0 ).unsqueeze(1).unsqueeze(3) # might be incorrect. see reason below\n",
    "        # unsqueeze(1).unsqueeze(3) might be incorrect for padding as it's different from unsqueeze(1).unsqueeze(2)\n",
    "        # so use the first one instead\n",
    "        target_mask = (target !=0 ).unsqueeze(1).unsqueeze(2)\n",
    "        seq_length = target.size(1)\n",
    "        \n",
    "        #Note that target_mask is (batch_size x 1 x 1 x seq_length) so,\n",
    "        #torch.ones(seq_length, seq_length) that will generate (seq_length x seq_length) instead of \n",
    "        #(1 x seq_length x seq_length) will also work!\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1,seq_length,seq_length),diagonal=1)).bool()\n",
    "        target_mask = target_mask & nopeak_mask\n",
    "        return source_mask, target_mask\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"\n",
    "        Args: N/A\n",
    "        \n",
    "        Return : N/A\n",
    "        \n",
    "        Explanation:\n",
    "        \n",
    "            This function calculates the number of parameters in the model\n",
    "        \"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    def forward(self,source,target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            source: Input tensor of size (batch_size x sequence length). Consists of index ids used to obtain\n",
    "                    embedding\n",
    "            target: Input tensor of size (batch_size x sequence length). Same indices as above.\n",
    "\n",
    "        Return:\n",
    "            output: Logits of size (batch_size x sequence length x target_vocabulary). This is then fed to \n",
    "                    softmax, so that for any given sequence in the batch, the next word at any position in \n",
    "                    that sequence can be obtained\n",
    "        \"\"\"\n",
    "        # Get the source_mask which masks out padded locations and get the target_mask which masks padded\n",
    "        # locations and look-ahead locations\n",
    "        source_mask,target_mask = self.generate_mask(source,target)\n",
    "        \n",
    "        # Obtain the source and target embeddings based on the ids\n",
    "        source_embed = self.positional_encoding(self.encoder_embedding(source))\n",
    "        target_embed = self.positional_encoding(self.decoder_embedding(target))\n",
    "        \n",
    "        # Execute the sequence of encoders\n",
    "        # First encoder takes the source_embeddings as input and subsequent encoders take the previous\n",
    "        # encoder's output as input\n",
    "        encoder_output = source_embed\n",
    "        for layer in self.encoder_layers:\n",
    "            encoder_output = layer(encoder_output,source_mask)\n",
    "        \n",
    "        # Execute the sequence of decoders\n",
    "        # For the first decoder, the masked multi-head attention takes the target_embedding as input\n",
    "        # and the subsquent decoders take the previous decoder's output as input\n",
    "        # For each decoder, the encoder-decoder or cross multi-head attention takes the output from the final\n",
    "        # encoder to calculate  Q & V values, and the output from the previous decoder to calculate the K values\n",
    "        decoder_output = target_embed\n",
    "        for layer in self.decoder_layers:\n",
    "            decoder_output = layer(decoder_output,encoder_output,source_mask,target_mask)\n",
    "        \n",
    "        # Final layer is a fully connected layer with the size of the target_vocabulary\n",
    "        # It outputs logits which are converted to probabilities by the Softmax portion of the CrossEntropy.\n",
    "        # Those proabibilities are then used to predict the next word/token at each position.\n",
    "        output = self.fc(decoder_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder_embedding): Embedding(5000, 512)\n",
      "  (decoder_embedding): Embedding(5000, 512)\n",
      "  (positional_encoding): PositionalEncoding()\n",
      "  (encoder_layers): ModuleList(\n",
      "    (0-5): 6 x EncoderBlock(\n",
      "      (mult_attn): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (feed_forward): PositionWiseFF(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0-5): 6 x DecoderBlock(\n",
      "      (mult_attn): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (cross_attn): MultiHeadAttention(\n",
      "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (feed_forward): PositionWiseFF(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm_3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=512, out_features=5000, bias=True)\n",
      ")\n",
      "Number of parameters:  51823496\n"
     ]
    }
   ],
   "source": [
    "# Create a transformer\n",
    "\n",
    "source_vocabsize = 5000\n",
    "target_vocabsize = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(source_vocabsize,target_vocabsize,d_model,num_heads,num_layers,\n",
    "                         d_ff,max_seq_length,dropout)\n",
    "\n",
    "#transformer.encoder_embedding.weight.requires_grad = False\n",
    "#transformer.decoder_embedding.weight.requires_grad = False\n",
    "\n",
    "print (transformer)\n",
    "print (\"Number of parameters: \",transformer.count_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
