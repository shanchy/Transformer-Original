{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original transformer architecture as per the \"Attention is all you need\" paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model,num_heads):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model : dimension of the embedding vectors\n",
    "            n_heads : number of self attention heads\n",
    "            \n",
    "        Return: N/A\n",
    "                    \n",
    "            \n",
    "        Explanation:\n",
    "            Why nn.Linear(d_model,d_model)? If d_model=512, that means a giant 512x512 weight matrix.\n",
    "            If d_model=512, num_heads = 8, then d_head = 64. Shouldn't it be nn.Linear(d_head,d_model)?\n",
    "            Then for an input I = 1x512, and weight matrix W = 64x512, doing I x W.transpose will give 1x64\n",
    "            \n",
    "            Reason is, I=1x512 with W=512x512 will give 1x512. This will be split into 8 parts,1 per head\n",
    "            That would mean the vector fed to each head would be 1x64.\n",
    "            So the computation is done one shot for efficiency, instead of multiply with 8 different matrices\n",
    "            of size 64x512. \n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads # Dimension of the Key, Query & Value vector passed to each head\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "    \n",
    "    def scaled_dot_product_attention (self,Q,K,V,mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Q,K,V : Query, Key & Value matrices. Dimension is (batch_size x num_heads x seq_length x d_head)\n",
    "            mask : masking locations for the decoder\n",
    "            \n",
    "        Return:\n",
    "            Z : Context vectors, from multiple self-attention blocks\n",
    "                Dimenesion is (batch_size x num_heads x seq_length x d_head)\n",
    "        \"\"\"\n",
    "        \n",
    "        #Compute attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(self.d_head)\n",
    "        \n",
    "        #Apply mask if available\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask==0, -1e9)\n",
    "            \n",
    "        #Compute softmax, row-wise, i.e. for a given row all column values are fed to the softmax\n",
    "        attn_probs = torch.softmax(attn_scores,dim=-1)\n",
    "        \n",
    "        #Compute output as summation of weighted V vectors\n",
    "        Z = torch.matmul(attn_probs,V)\n",
    "        return Z\n",
    "    \n",
    "    def split_heads(self,x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x : A tensor representing either Q,K or V, obtained after applying the corresponding W_q, W_k or\n",
    "                W_v to the input batch. Size is (batch_size x seq_length x d_model)\n",
    "        Return: A reordered tensor of size (batch_size x num_heads x seq_length x d_head), which was originally\n",
    "                (batc_size x seq_length x num_heads x d_head). This was resized from the input.\n",
    "                num_heads x d_head = d_model\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view (batch_size, seq_length, self.num_heads, self.d_head).transpose(1,2)\n",
    "    \n",
    "    def combine_heads(self,x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x : A tensor representing the computed context vectors \n",
    "                Dimensions are (batch_size x num_heads x seq_length x d_head)\n",
    "        Return: This function does the reverse operation of split_heads. Therefore, it takes the input and \n",
    "                creates (batch_size x seq_length x d_model)\n",
    "        \"\"\"\n",
    "        batch_size, _, seq_length,d_head = x.size()\n",
    "        return x.transpose(1,2).contiguous().view(batch_size,seq_length,self.d_model)\n",
    "    \n",
    "    \n",
    "    def forward(self,Q,K,V, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Q,K,V : All there parameters get a copy of the input batch as an argument.\n",
    "                    Dimension of input batch is (batch_size x seq_length x d_model)\n",
    "            mask : masking locations - padding if encoder, and padding+look-ahead if decoder\n",
    "        Return:\n",
    "            output : A tensor representing the output of the multi-head self-attention block\n",
    "                    Dimension is (batch_size x seq_length x d_model)\n",
    "        \n",
    "        \n",
    "        Explanation:\n",
    "            Input is a batch of samples. \n",
    "            Assume batch_size = 32. Each sample is a sequence of tokens. Assume seq_length = 16. Each token\n",
    "            will be converted to a embedding. Assume embedding size = 512. \n",
    "            Then, after the embedding is generated for a batch of 32 samples and the positional embeddings are\n",
    "            added, each batch has dimensions (32x16x512) \n",
    "            \n",
    "            ========== Compute the Q, K & V vectors ============\n",
    "            \n",
    "            The weight matrices W_q, W_k & W_v are all 512x512 (see reason in __init__ method). \n",
    "            Q, K & V are computed by apply the corresponding weight matrices to a copy of the input batch. \n",
    "            The resultant matrix's dimensions remains same as the input batch at 32x16x512.\n",
    "            \n",
    "            Each matrix is then resized to (32x16x8x64). This means that for each of the 16 tokens, there are \n",
    "            8 vectors, each of them 64-D, which will be fed in parallel to the 8 attention heads.\n",
    "            For efficient computation, each matrix is rearranged to (32x8x16x64). Now, for each of the 8 heads,\n",
    "            there are 16 vectors (corresponding to 16 tokens in each sequence) of 64-D each.\n",
    "            In other words, each of the 8 attention heads will receive a batch of 32 tensors, where each \n",
    "            tensor will consist of 16 vectors, each of dimension 64-D.\n",
    "        \n",
    "            ========== Compute multi-head self-attention context vectors =========\n",
    "            Attention scores are computed by multiplying K = (32x8x16x64) by transpose of Q = (32x8x64x16) \n",
    "            Note that prior to transpose it was (32x8x16x64). Resultant matrix is 32x8x16x16\n",
    "            This score is rescaled by dividing with sqrt(d_head), where d_head = 64. Next softmax is applied.\n",
    "            Dimensions don't change during the rescaling and softmax\n",
    "            Next the matrix is multiplied by the value matrix. So (32x8x16x16) x (32x8x16x64) -> (32x8x16x64).\n",
    "            This multiplication perform 2 actions to compute the final context vector for each token,: \n",
    "            a) it multiplies the V vectors with the computed attention scores, and\n",
    "            b) sums the vectors.\n",
    "            This is done across all 8 heads in parallel, generating 8 context vectors (64-D) per token \n",
    "            \n",
    "            ========== Compute combined output ===========\n",
    "            The (32x8x16x64) is reversed back to (32x16x8x64) which is (batch_size x seq_length x num_heads\n",
    "            x d_head). It is then rearranged so that for each token, the 8 vectors of 64-D dimension are \n",
    "            concatenated to form a 512 vector, resulting in (32x16x512). This is then passed through a liner\n",
    "            layer with weight matrix 512x512. The final tensor is (32x16x512)\n",
    "            \n",
    "        \"\"\"\n",
    "    \n",
    "        # Compute the Q, K & V vectors \n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Compute multi-head self-attention context vectors\n",
    "        attn_ctxt_vecs = self.scaled_dot_product_attention(Q,K,V, mask)\n",
    "        \n",
    "        # Compute combined output\n",
    "        output = self.W_o(self.combine_heads(attn_ctxt_vecs))\n",
    "        \n",
    "        return(output)\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feed-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFF(nn.Module):\n",
    "    def __init__(self,d_model,d_ff):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model : dimension of the multi-head self-attention output vector\n",
    "            d_ff : dimension of the inner Linear layer, usually much smaller than d_model\n",
    "            \n",
    "        Return: N/A\n",
    "        \"\"\"\n",
    "        super(PositionWiseFF,self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model,d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x : output of the multi-head self-attention block. Size is (batch_size x seq_length x d_model) \n",
    "            \n",
    "        Return: Tensor, the same size as the input x\n",
    "        \n",
    "        Explanation:\n",
    "            Position-wise means that every single token representation will be fed to the neural net.\n",
    "            Feed-forward networks usually comprise of two Linear layers. The first one expands the dimension and\n",
    "            the second one reduces it back\n",
    "            \n",
    "            It is important to note that even though the input d_model is exactly the same as the input \n",
    "            embedding dimension, this is not compulsory. Depending on the weight matrices in the attention \n",
    "            block, this d_model dimension could be smaller/larger compared to the input embedding dimension.\n",
    "            Similarly, the second layer reduces back to the same size as the input. This is also not compulsory\n",
    "            For the sake of simplicity, in this architecture, dimension of d_model is used everywhere.\n",
    "        \"\"\"\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model,max_seq_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model : input embedding dimension\n",
    "            max_seq_length : maximum number of tokens in an input sample\n",
    "        \n",
    "        Return : N/A\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        \n",
    "        # A vector representing the index positions of each token in the sequence, reshaped to a column vector\n",
    "        position = torch.arange(0,max_seq_length,dtype=torch.float).unsqueeze(1)\n",
    "   \n",
    "        # A curve that starts at 1 and decays exponentially towards 0, index increases from 0 -> d_model, at \n",
    "        # interval of 2\n",
    "        div_term = torch.exp(torch.arange(0,d_model,2).float() * -(math.log(10000.0)/d_model))\n",
    "        \n",
    "        # Input embedding has dimension d_model.\n",
    "        # To compute the corresponding positional embedding of similar dimension,\n",
    "        # For any given even position i in the embedding, two values are computed\n",
    "        # 1) Value for that position i = Sin of the div_term, based on that particular position i\n",
    "        # 2) Value of the next position i+1 = Cos of the div_term, based on that same position i\n",
    "        # The position refers to the position of each token in the sequence.\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Unsqueezing helps to make the tensor the same size as the input\n",
    "        self.register_buffer('pe',pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x : input embeddings.Size is (batch_size x seq_length x d_model)\n",
    "        \n",
    "        Return: Tensor, the same size as x\n",
    "        \n",
    "        Explanation:\n",
    "        \n",
    "            The original positional encoding formula is as follows:\n",
    "            PE(pos,2i)     = sin(pos/10000^((2*i)/d_model))\n",
    "            PE(pos,2i + 1) = cos(pos/10000^((2*i)/d_model))\n",
    "        \n",
    "            In this code, the following formula is used\n",
    "            PE(pos,2i)     = sin(pos/e^((i*log(10000))/d_model))\n",
    "            PE(pos,2i + 1) = cos(pos/e^((i*log(10000))/d_model))\n",
    "            \n",
    "            So the primary difference is that for the denominator,\n",
    "            e^((i*log(10000))/d_model) is used instead of 10000^((2*i)/d_model).\n",
    "            However, both of them have similar exponential decay behaviour from 1 towards 0 with as\n",
    "            index increase from 0 -> d_model.\n",
    "            \n",
    "            So the overall behaviour is tha the sine will approach 0 and the cos will approach 1.\n",
    "            The pos value increases the frequency and therefore controls how many oscillations occur before\n",
    "            the curves approach 0/1. \n",
    "            \n",
    "            It is this difference in frequency that makes each curve unique and hence gives each token a unique\n",
    "            embedding based on the token position!\n",
    "            \n",
    "            Note:\n",
    "            The div_term can also be computed as follows.\n",
    "            div_term = 1/torch.exp((torch.arange(0, 50, 2).float() * math.log(10000.0))/50)\n",
    "            \n",
    "        \"\"\"\n",
    "        return x + self.pe[:,:x.size(1)] # Sequence length is the second dimension (dim=1) of x\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,d_model,num_heads,d_ff,dropout):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model : dimension of the input samples\n",
    "            num_heads : number of attention heads\n",
    "            d_ff : number of nodes in the hidden layer of the feed forward network\n",
    "            droput : dropout ratio\n",
    "            \n",
    "        Return: N/A\n",
    "        \"\"\"\n",
    "        super(EncoderBlock,self).__init__()\n",
    "        self.mult_attn = MultiHeadAttention(d_model,num_heads)\n",
    "        self.feed_forward = PositionWiseFF(d_model,d_ff)\n",
    "        self.norm_1 = nn.LayerNorm(d_model)\n",
    "        self.norm_2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self,x,mask):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            x : Input to the encoder. Size is usually (batch_size x seq_length x d_model)\n",
    "            mask : specifies which token positions in the sequence are padded\n",
    "\n",
    "        Return:\n",
    "            x : an output tensor, similar in size to the input x\n",
    "            \n",
    "        Explanation:\n",
    "            Each encoder block has 2 main components\n",
    "            a) The multi-head self attention\n",
    "            b) The feed forward network\n",
    "            \n",
    "            First, the multi-head attention is computed for the encoder input. Any necessary padding masks are\n",
    "            specified. Add and norm is performed on the output\n",
    "            This is then fed to the position-wise feed forward network. Add and norm is peformed on the output\n",
    "        \"\"\"\n",
    "        attention_output = self.mult_attn(x,x,x,mask)\n",
    "        x = self.norm_1(x + self.dropout(attention_output))\n",
    "        feedforward_output = self.feed_forward(x)\n",
    "        x  = self.norm_2(x + self.dropout(feedforward_output))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,d_model,num_heads,d_ff,dropout):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model : dimension of the input samples\n",
    "            num_heads : number of attention heads\n",
    "            d_ff : number of nodes in the hidden layer of the feed forward network\n",
    "            droput : dropout ratio\n",
    "            \n",
    "        Return: N/A\n",
    "        \"\"\"\n",
    "        super(DecoderBlock,self).__init__()\n",
    "        self.mult_attn = MultiHeadAttention(d_model,num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model,num_heads)\n",
    "        self.feed_forward = PositionWiseFF(d_model,d_ff)\n",
    "        self.norm_1 = nn.LayerNorm(d_model)\n",
    "        self.norm_2 = nn.LayerNorm(d_model)\n",
    "        self.norm_3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x,encoder_output, source_mask, target_mask):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            x : Input to the decoder. Size is usually (batch_size x seq_length x d_model)\n",
    "            encoder_output : Output from the final encoder block. Size is (batch_size x seq_length x d_model)\n",
    "            source_mask : specifies which token positions in the input to encoder is padded\n",
    "            target_mask : specifies the look-ahead masks for the input to the decoder \n",
    "\n",
    "        Return:\n",
    "             : an output tensor, similar in size to the input x\n",
    "             \n",
    "        Explanation:\n",
    "            Each decoder block has 3 main components\n",
    "            a) The masked multi-head self attention\n",
    "            b) The normal multi-head attention\n",
    "            c) The position-wise feed forward network\n",
    "            \n",
    "            First, the masked multi-head attention is computed for the decoder input. During training, \n",
    "            the decoder input is the target sequence, but specified with the look-ahead mask. The look-ahead\n",
    "            mask ensures that for each current token in the target sequence, it only has access to information \n",
    "            from preceding tokens. Succeeding tokens are masked and hence cannot contribute to the computations\n",
    "            of the current token.             \n",
    "            Add and norm is performed on the output of the masked multi-head attention.\n",
    "            Next, the output is fed to a cross multi-head attention and used to compute K. \n",
    "            This cross multi-head attention also receives the outputs from the final encoder block, which\n",
    "            are used to compute Q & V.\n",
    "            Add and norm is performed on the output of the cross multi-head attention.\n",
    "            This is then fed to the position-wise feed forward network. Add and norm is peformed on the output\n",
    "        \"\"\"\n",
    "        attention_output = self.mult_attn(x,x,x,target_mask)\n",
    "        x = self.norm_1(x + self.dropout(attention_output))\n",
    "        cross_attention_output = self.cross_attn(x,encoder_output,encoder_output,source_mask)\n",
    "        x = self.norm_2(x + self.dropout(cross_attention_output))\n",
    "        feedforward_output = self.feed_forward(x)\n",
    "        x  = self.norm_3(x + self.dropout(feedforward_output))\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The transformer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,source_vocabsize,target_vocabsize,\n",
    "                 d_model,num_heads,num_layers,d_ff,max_seq_length,droput):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            source_vocabsize : size of the source vocabulary\n",
    "            target_vocabsize : size of the target vocabulary\n",
    "            d_model : dimension of input embedding\n",
    "            num_heads : number of self-attention heads\n",
    "            num_layers : number of encoder and/or decoder blocks\n",
    "            d_ff : number of nodes in the inner layer of the feed-forward network\n",
    "            max_seq_length: max length of an input/output sequence\n",
    "            droput : dropout ratio\n",
    "\n",
    "        Return: N/A\n",
    "        \n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(source_vocabsize, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(target_vocabsize, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model,max_seq_length)\n",
    "        self.encoder_layers = nn.ModuleList([EncoderBlock(d_model,num_heads,d_ff,droput) \n",
    "                                            for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderBlock(d_model,num_heads,d_ff,droput) \n",
    "                                            for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(d_model,target_vocabsize)\n",
    "    \n",
    "    def generate_mask(self,source,target,pad_idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            source: Input tensor of size (batch_size x sequence_length). Each row consists of a sequence of \n",
    "                    vocabulary indices, which is used to obtain embeddings. If the number of tokens in the \n",
    "                    sequence is less than \"sequence_length\", the remaining positions have a <pad> token, represented\n",
    "                    by pad_idx, usually 0\n",
    "                    \n",
    "                    Eg, \n",
    "                    sequence length is 16.\n",
    "                    let sample be the following sentence \"I like cats and dogs, but I prefer dogs\". Length = 10.\n",
    "                    Therefore, the remaining 6 positionos are padded as follows\n",
    "                    I like cats and dogs , but I prefer dogs <pad> <pad> <pad> <pad> <pad> <pad>\n",
    "                    Converted to indices: 32 458 9128 27 9456 5 46 32 7321 9456 0 0 0 0 0 0\n",
    "                    \n",
    "            target: Input tensor of size (batch_size x sequence_length). Details, similar to source\n",
    "            pad_idx : index value representing the pad token\n",
    "\n",
    "        Return:\n",
    "            source_mask: Tensor of boolean values, where padded positions are masked. The tensor will have \n",
    "                         dimensions such that it can be applied to the encoder multi-head attention scores\n",
    "                         Dimension will be (batch_size x 1 x 1 x sequence_length). See Explanation\n",
    "            target_mask: Tensor of boolean values, where padded and look-ahead positions are masked. Tensor\n",
    "                         dimensions must match the masked multi-head attentions scores and the encoder-decoder\n",
    "                         multi-head attention scores.\n",
    "                         Dimension will be (batch_size x 1 x sequence_length x sequence_length). See Explanation\n",
    "                         \n",
    "        Explanation:\n",
    "            Let's assume a batch size of 3, with sequence length of 5. \n",
    "            So input size is (batch_size x sequence_length) = (3 x 5)\n",
    "            \n",
    "            source = [[35, 87, 234, 1233, 0],\n",
    "                      [56, 12, 231, 2323, 722], \n",
    "                      [9121, 444, 0, 0 ,0]]\n",
    "                      \n",
    "            target = [[723, 823, 31, 0, 0],\n",
    "                      [981, 323, 3095, 31, 0],\n",
    "                      [91, 8756, 8123, 231, 88]]\n",
    "                      \n",
    "            =====================================          \n",
    "            First, the source_mask needs to be obtained. The source values are converted to boolean, where False indicates\n",
    "            masked positions, as follows\n",
    "            [[ True,  True,  True,  True, False],    <-- sample 1\n",
    "             [ True,  True,  True,  True,  True],    <-- sample 2\n",
    "             [ True,  True, False, False, False]].   <-- sample 3\n",
    "             \n",
    "             This tensor is the same size as the input, which is (3 x 5). However it needs to be adjusted so that\n",
    "             it can be applied to the attention scores.\n",
    "             Assuming, there are 8 heads, the tensor of attention scores will be (3 x 8 x 5 x 5)\n",
    "             \n",
    "             Performing unsqueeze(1), follwed by unsqueeze(2), converts the (3x5) mask to (3 x 1 x 1 x 5).\n",
    "             Now this can be applied to the attention scores and is returned by the function\n",
    "            \n",
    "             BUT HOW WILL IT BE APPLIED TO THE ATTENTION SCORES???\n",
    "             \n",
    "             Let's look at sample 1 mask = True, True, True, True, False. This means the 5th position is masked\n",
    "             Remember that for each sample, there are 8 heads, and each head has attention scores of 5x5,\n",
    "             and hence the last three dimension of 8x5x5 in (3x8x5x5).\n",
    "             During the mask application, the boolean value is broadcasted across the second dimension to become\n",
    "             \n",
    "             [[True, True, True, True, False],\n",
    "              [True, True, True, True, False],\n",
    "              [True, True, True, True, False],\n",
    "              [True, True, True, True, False],\n",
    "              [True, True, True, True, False]].\n",
    "              \n",
    "              Do notice how the 5th position that was padded always gets masked out.\n",
    "              \n",
    "              This is then broadcasted across the 3rd dimension, so that each of the heads for the 1st sample\n",
    "              gets this same 5x5 tensor as the mask.\n",
    "              \n",
    "              This broadcasting occurs in this manner due to the dimensionality of 1 in (3 x 1 x 1 x 5).\n",
    "              \n",
    "              In similar fashion, the 2nd sample is as follows after broadcasting\n",
    "              \n",
    "              [[ True,  True,  True,  True,  True],\n",
    "               [ True,  True,  True,  True,  True],\n",
    "               [ True,  True,  True,  True,  True],\n",
    "               [ True,  True,  True,  True,  True],\n",
    "               [ True,  True,  True,  True,  True]]\n",
    "              \n",
    "               and the 3rd sample is \n",
    "               [[ True,  True, False, False, False],\n",
    "                [ True,  True, False, False, False],\n",
    "                [ True,  True, False, False, False],\n",
    "                [ True,  True, False, False, False],\n",
    "                [ True,  True, False, False, False]]\n",
    "             \n",
    "                Take not how the 2nd sample has no padding, whereas the 3rd sample always has the 3rd - 5th\n",
    "                positions padded.\n",
    "                \n",
    "                ============================\n",
    "                Next, the target mask needs to obtained.\n",
    "                The target mask goes through the same masking process for padded tokens as the source mask, \n",
    "                but, it also has to go through look-ahead masking.\n",
    "                \n",
    "                When applying masking to the target for the padded tokens, the target_mask is as following.\n",
    "                \n",
    "                [[ True,  True,  True,  False, False],    <-- sample 1\n",
    "                 [ True,  True,  True,  True,  False],    <-- sample 2\n",
    "                 [ True,  True,  True,  True,  True]].    <-- sample 3\n",
    "                 \n",
    "                Similar to the source, this is (3 x 1 x 1 x 5) and will be applied to the attention scores via\n",
    "                broadcasting.\n",
    "                \n",
    "                The look head mask, called the nopeak_mask is based on the sequence length is obtained by\n",
    "                a function that applies masks diagonally. In this case, the sequence length is 5, hence\n",
    "                nopeak_mask is a follows:\n",
    "                \n",
    "                [[ True, False, False, False, False],  \n",
    "                 [ True,  True, False, False, False],\n",
    "                 [ True,  True,  True, False, False],\n",
    "                 [ True,  True,  True,  True, False],\n",
    "                 [ True,  True,  True,  True,  True]]\n",
    "                 \n",
    "                 Notice the diagonal nature. When this mask is applied to the attention score of (5x5),it means\n",
    "                 that at any given position in that sequence, the tokens only have access to information from\n",
    "                 itself and previous tokens, not the tokens after it. During training, the target is actually fed\n",
    "                 to the decoder, and therefore this masking is mandatory because the model should not be able to \n",
    "                 see the tokens that it is going to predict. \n",
    "                 \n",
    "                 The final target mask is obtained by doing target_mask & nopeak_mask\n",
    "                 Note that this nopeak_mask can be (5x5) or (1x5x5). Either way it is compatible with the\n",
    "                 target_mask which is (3x1x1x5) \n",
    "                 \n",
    "                 Do notice that during this & operation, the values in the 1st dimension of the target_mask\n",
    "                 are broadcasted across the second dimension to get (3x1x5x5) to match(1x5x5).\n",
    "                 \n",
    "                 After the & operation, the resultant target mask becomes a (3x1x5x5):\n",
    "                 \n",
    "                 [[[[ True, False, False, False, False],\n",
    "                    [ True,  True, False, False, False],     \n",
    "                    [ True,  True,  True, False, False],     5x5 tensor for sample 1, applied to all 8 heads\n",
    "                    [ True,  True,  True, False, False],     Notice the last 2 positions are always masked,\n",
    "                    [ True,  True,  True, False, False]]],   in addition to the look-ahead mask\n",
    "\n",
    "\n",
    "                  [[[ True, False, False, False, False],\n",
    "                    [ True,  True, False, False, False],\n",
    "                    [ True,  True,  True, False, False],     5x5 tensor for sample 2, applied to all 8 heads\n",
    "                    [ True,  True,  True,  True, False],     Notice the last position is always masked,\n",
    "                    [ True,  True,  True,  True, False]]],   in addition to the look-ahead mask\n",
    "\n",
    "\n",
    "                  [[[ True, False, False, False, False],\n",
    "                    [ True,  True, False, False, False],\n",
    "                    [ True,  True,  True, False, False],    5x5 tensor for sample 3 applied to all 8 heads\n",
    "                    [ True,  True,  True,  True, False],    Notice no position is always masked. \n",
    "                    [ True,  True,  True,  True,  True]]]]  Only the look-ahead mask exists\n",
    "                    \n",
    "                This (3x1x5x5) tensor is the target_mask is returned by the function\n",
    "                     \n",
    "        \"\"\"\n",
    "        source_mask = (source != pad_idx ).unsqueeze(1).unsqueeze(2)\n",
    "        # target_mask = (target != pad_idx ).unsqueeze(1).unsqueeze(3) # might be incorrect. see reason below\n",
    "        # unsqueeze(1).unsqueeze(3) might be incorrect for padding as it's different from unsqueeze(1).unsqueeze(2)\n",
    "        # so use the first one instead\n",
    "        target_mask = (target != pad_idx ).unsqueeze(1).unsqueeze(2)\n",
    "        seq_length = target.size(1)\n",
    "        \n",
    "        #Note that target_mask is (batch_size x 1 x 1 x seq_length) so,\n",
    "        #torch.ones(seq_length, seq_length) that will generate (seq_length x seq_length) instead of \n",
    "        #(1 x seq_length x seq_length) will also work!\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1,seq_length,seq_length),diagonal=1)).bool()\n",
    "        target_mask = target_mask & nopeak_mask\n",
    "        return source_mask, target_mask\n",
    "    \n",
    "    def encode(self, source, source_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            source : Input of size (batch_size x sequence_length), where each row is a sequence of token ids\n",
    "            source_mask : A tensor where padded positions in the source are masked. This tensor is resized\n",
    "                          to match the multi-head attention scores. See function generate_mask for example\n",
    "        \n",
    "        Return:\n",
    "            encoder_output : Output from the last encoder block. \n",
    "                             Size is (batch_size x sequence_length x d_model)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Obtain the source embeddings based on the ids\n",
    "        source_embed = self.positional_encoding(self.encoder_embedding(source))\n",
    "        \n",
    "        # Execute the sequence of encoder blocks\n",
    "        # First encoder block takes the source_embeddings as input and subsequent encoder blocks take the \n",
    "        # previous encoder block's output as input\n",
    "        encoder_output = source_embed\n",
    "        for layer in self.encoder_layers:\n",
    "            encoder_output = layer(encoder_output,source_mask)\n",
    "        \n",
    "        return encoder_output\n",
    "    \n",
    "    def decode(self, target,encoder_output,source_mask,target_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            target: Input of size (batch_size x sequence_length), where each row is a sequence of token ids\n",
    "            encoder_output : Output from the encoder function\n",
    "            source_mask : A tensor where padded positions in the source are masked. This tensor is resized\n",
    "                          to match the multi-head attention scores. See function generate_mask for example\n",
    "            target_mask : A tensor where padded positions & look-ahead positions in the source are masked. \n",
    "                          This tensor is resized to match the multi-head attention scores. \n",
    "                          See function generate_mask for example\n",
    "        Return: \n",
    "            decoder_output : Output from the last decoder block. \n",
    "                             Size is (batch_size x sequence_length x d_model)\n",
    "        \n",
    "        \"\"\"\n",
    "        # Obtain the target embeddings based on the ids\n",
    "        target_embed = self.positional_encoding(self.decoder_embedding(target))\n",
    "        \n",
    "        # Execute the sequence of decoder blocks\n",
    "        # For the first decoder block, the masked multi-head attention takes the target_embedding as input\n",
    "        # and the subsquent decoder blocks take the previous decoder block's output as input\n",
    "        # For each decoder block, the encoder-decoder or cross multi-head attention takes the output from the \n",
    "        # final encoder block to calculate  Q & V values, and the output from the previous decoder block \n",
    "        # to calculate the K values\n",
    "        decoder_output = target_embed\n",
    "        for layer in self.decoder_layers:\n",
    "            decoder_output = layer(decoder_output,encoder_output,source_mask,target_mask)\n",
    "        \n",
    "        return decoder_output\n",
    "    \n",
    "        \n",
    "    def do_inference(self, src, src_mask, sos_idx, eos_idx, max_len=50):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src : Input of size (batch_size x sequence_length), where each row is a sequence of token ids\n",
    "            src_mask : A tensor where padded positions in the source are masked. This tensor is resized\n",
    "                       to match the multi-head attention scores. See function generate_mask for example\n",
    "            sos_idx : index of the sos token\n",
    "            eos_idx : index of the eos token\n",
    "            max_len : maximum sequence length\n",
    "            \n",
    "        Return:\n",
    "            preds : Output of size (batch_size x sequence_length), where each row is a sequence of token ids\n",
    "            \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # Obtain the number of samples\n",
    "        batch_size = src.shape[0]\n",
    "        \n",
    "        # Create an array of booleans that indicates whether a particular sample/sequence has finished decoding\n",
    "        # Initialize to False, and assign True when done decoding\n",
    "        done = {i:False for i in range(batch_size)}\n",
    "        \n",
    "        # Set to evaluation mode\n",
    "        self.eval()\n",
    "        \n",
    "        # Initialize the predictions for all samples with the index of the SOS token\n",
    "        preds = torch.LongTensor([[sos_idx]] * batch_size).to(src.device)\n",
    "        \n",
    "        # Run the samples through the transformer encoder and obtain the encoder outputs\n",
    "        encoder_output = self.encode(src, src_mask)\n",
    "        \n",
    "        # Run the decoder iteratively and obtain the index of the next token in each iteration, until the \n",
    "        # EOS token is obtained.\n",
    "        for _ in range(max_len-1):\n",
    "            \n",
    "            # Run the decoder based on the encoder_outputs & all the predictions till the current iteration\n",
    "            decoder_output = self.decode(preds,encoder_output,src_mask,None)\n",
    "            \n",
    "            # Obtain the logits based on the final fully connected layer\n",
    "            logits = self.fc(decoder_output)\n",
    "            \n",
    "            # The dimension of the logits is (batch_size,sequence_length,vocab_size)\n",
    "            # For each sample in the batch, take the last row and find the index corresponding to the max value \n",
    "            # in that row. That is the index of the next token.\n",
    "            next_idx = torch.max(logits[:,-1:],dim=-1).indices\n",
    "            \n",
    "            # Update the predictions by appending these indices\n",
    "            preds = torch.concat((preds,next_idx),dim=1)\n",
    "            \n",
    "            # Mark a sample as done if index of EOS token is obtained in current iteration.\n",
    "            # Exit loop if all samples are marked as done\n",
    "            for i,idx in enumerate(next_idx):\n",
    "                if idx[0] == eos_idx:\n",
    "                    done[i] = True\n",
    "            if False not in done.values():\n",
    "                break\n",
    "               \n",
    "        # Set to training mode\n",
    "        self.train()\n",
    "        return preds\n",
    "            \n",
    "    def count_parameters(self):\n",
    "        \"\"\"\n",
    "        Args: N/A\n",
    "        \n",
    "        Return : N/A\n",
    "        \n",
    "        Explanation:\n",
    "        \n",
    "            This function calculates the number of parameters in the model\n",
    "        \"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    \n",
    "    def forward(self,source,target,pad_idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            source: Input tensor of size (batch_size x sequence length). Consists of index ids used to obtain\n",
    "                    embedding\n",
    "            target: Input tensor of size (batch_size x sequence length). Same indices as above.\n",
    "\n",
    "        Return:\n",
    "            output: Logits of size (batch_size x sequence length x target_vocabulary). This is then fed to \n",
    "                    softmax, so that for any given sequence in the batch, the next word at any position in \n",
    "                    that sequence can be obtained\n",
    "        \"\"\"\n",
    "        # Get the source_mask which masks out padded locations and get the target_mask which masks padded\n",
    "        # locations and look-ahead locations\n",
    "        source_mask,target_mask = self.generate_mask(source,target,pad_idx)\n",
    "        \n",
    "        # Execute the encoder\n",
    "        encoder_out = self.encode(source,source_mask)\n",
    "        \n",
    "        # Execute the decoder\n",
    "        decoder_out = self.decode(target,encoder_out,source_mask,target_mask)\n",
    "        \n",
    "        # Final layer is a fully connected layer with the size of the target_vocabulary\n",
    "        # It outputs logits which are converted to probabilities by the Softmax portion of the CrossEntropy.\n",
    "        # Those proabibilities are then used to predict the next word/token at each position.\n",
    "        output = self.fc(decoder_out)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & test the transformer functionality on a simple task : Reverse a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Specify the BOS,SOS & PAD indices\n",
    "bos_idx, eos_idx, pad_idx = 1, 2, 0\n",
    "# Use 100 integers and max length of 16 only.\n",
    "vocab_size, src_len = 100, 16\n",
    "\n",
    "# Create a data loaoder.\n",
    "data_loader = data.DataLoader( \n",
    "    # Create a dataset of number sequences, where each sequence can be 8 - 16 numbers long\n",
    "    dataset=[torch.randint(3,vocab_size,(torch.randint(src_len//2,src_len,(1,)),)) for _ in range(50000)],\n",
    "    # Specify batch size, shuffle the samples and drop the last batch if it is not equal to batch_size\n",
    "    batch_size=128, shuffle=True, drop_last=True,\n",
    "    # For each sample in a batch, create the source and target sequences for training\n",
    "    # For the source, take each sample in the batch and pad with 'pad_idx', if sequence length is less than 16\n",
    "    # For the target, reverse each sample in the batch, encapsulate with 'bos_idx' & 'eos_idx', and do padding\n",
    "    collate_fn=lambda batch: (\n",
    "        # SOURCE SAMPLES FOR THE BATCH\n",
    "        pad_sequence(batch, batch_first=True, padding_value=pad_idx), \n",
    "        \n",
    "        # TARGET SAMPLES FOR THE BATCH\n",
    "        pad_sequence([torch.LongTensor([bos_idx] + x.flip(0).tolist() + [eos_idx]) for x in batch],\n",
    "            batch_first=True, padding_value=pad_idx,)),\n",
    "    )\n",
    "\n",
    "# Create a small transformer as this is a simple task. Start by specifying the necessary transformer parameters\n",
    "source_vocabsize = 100\n",
    "target_vocabsize = 100\n",
    "d_model = 64\n",
    "num_heads = 2\n",
    "num_layers = 2\n",
    "d_ff = 128\n",
    "max_seq_length = 32\n",
    "dropout = 0.1\n",
    "\n",
    "# Instantiate the transformer\n",
    "seq_reverser = Transformer(source_vocabsize,target_vocabsize,d_model,num_heads,num_layers,\n",
    "                         d_ff,max_seq_length,dropout)\n",
    "# Instantiate the optimizer\n",
    "optim = torch.optim.AdamW(seq_reverser.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "steps_per_epoch = len(data_loader)\n",
    "# Train the transformer for 15 epochs.\n",
    "num_epochs = 15\n",
    "for epoch_index in range(num_epochs):\n",
    "    running_train_loss = 0\n",
    "    i = 0 # number of training iterations per epoch\n",
    "    for src, tgt in data_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        # Previous outputs are fed back as input into the decoder, and the transformer keeps predicting the \n",
    "        # next output in the sequence until the EOS is reached.\n",
    "        # tgt_in is fed into the decoder and tgt_out is compared against the actual transformer outputs\n",
    "        tgt_in, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
    "        # Forward propagation\n",
    "        logits = seq_reverser(src, tgt_in, pad_idx) \n",
    "        # Calculate the loss\n",
    "        loss = nn.functional.cross_entropy(logits.permute(0,2,1), tgt_out, ignore_index=pad_idx)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        # Backward Propagation of errors\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(seq_reverser.parameters(), 1.)\n",
    "        # Update model parameters\n",
    "        optim.step()\n",
    "        \n",
    "        i=i+1 \n",
    "        print (f\"Completed training step: {i}/{steps_per_epoch} in epoch: {epoch_index+1}.Training loss: {loss}\",end='\\r')\n",
    "        running_train_loss+=loss.item()\n",
    "    print(\"Training loss : \",running_train_loss/steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input   =  tensor([[56, 71, 23, 44, 91, 92, 93, 55, 56, 57, 83, 71, 33]])\n",
      "Output  =  tensor([[33, 71, 83, 57, 56, 55, 93, 92, 91, 44, 23, 71, 56]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.LongTensor([56, 71, 23,44, 91, 92, 93, 55, 56,57,83,71,33 ]).unsqueeze(dim=0).to(device)\n",
    "y = seq_reverser.do_inference(x, None, bos_idx, eos_idx, max_len=32)\n",
    "print(\"Input   = \", x) # Print the input sequence\n",
    "print(\"Output  = \",y[:,1:-1]) # Prin the reversed sequence, but exclude BOS & EOS indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, train & test the transformer on a more realistic task : Machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the following packages, if any of them fail during import\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.datasets import Multi30k \n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab\n",
    "from collections import Counter\n",
    "\n",
    "# Load the german & english datasets from Multi30k as source and target sets, respectively\n",
    "train_data = Multi30k(root=\"datasets\", split=\"train\", language_pair=(\"de\",\"en\"))\n",
    "# Convert the training set to a list of tuples. Each tuple has a german sentence as source & english sentence\n",
    "# as target\n",
    "train_data = [(src, tgt) for src, tgt in train_data if len(src) > 0]\n",
    "\n",
    "UNK, PAD, BOS, EOS = (\"<UNK>\", \"<PAD>\", \"<START>\", \"<END>\") # Specify the special tokens\n",
    "\n",
    "# this tokenizer just splits the sentence into words & converts upper caps to lower caps\n",
    "tokenizer = get_tokenizer(\"basic_english\") \n",
    "\n",
    "# For each language, create a counter to count the number of occurences of each word\n",
    "en_counter, de_counter = Counter(), Counter() \n",
    "\n",
    "# Go through the german and english sentences & update their respective counters\n",
    "for src, tgt in train_data:\n",
    "    de_counter.update(tokenizer(src))\n",
    "    en_counter.update(tokenizer(tgt))\n",
    "\n",
    "# Create the vocabularies of each language based on the counters & the special tokens\n",
    "# The vocabulary is a {word,index} dictionary\n",
    "de_vocab = vocab(de_counter, specials=[UNK, PAD, BOS, EOS])\n",
    "de_vocab.set_default_index(de_vocab[UNK])\n",
    "en_vocab = vocab(en_counter, specials=[UNK, PAD, BOS, EOS])\n",
    "en_vocab.set_default_index(en_vocab[UNK])\n",
    "pad_idx = de_vocab[PAD] # pad_idx is 1\n",
    "assert en_vocab[PAD] == de_vocab[PAD]\n",
    "\n",
    "\n",
    "# Create the dataloader\n",
    "train_loader = data.DataLoader(\n",
    "    dataset=train_data,\n",
    "     # Specify batch size, shuffle the samples and drop the last batch if it is not equal to batch_size\n",
    "    batch_size=128, shuffle=True, drop_last=True,\n",
    "    # For each sample in a batch, create the source and target sentences for training\n",
    "    # For the source, take each sample in the batch, tokenize it and then convert it to a sequence of indices,\n",
    "    # and pad with 'pad_idx', if sequence length is less than the longest sentence in that batch\n",
    "    # For the target, take each sample in the batch, and apply all operations as the source, but encapsulate\n",
    "    # the sequence with the BOS & EOS indices before padding\n",
    "    collate_fn=lambda batch: (\n",
    "        pad_sequence(\n",
    "            [torch.LongTensor(de_vocab(tokenizer(x))) for x, _ in batch],\n",
    "            batch_first=True, padding_value=pad_idx),\n",
    "        pad_sequence(\n",
    "            [torch.LongTensor(en_vocab([BOS] + tokenizer(y) + [EOS])) for _, y in batch],\n",
    "            batch_first=True, padding_value=pad_idx),\n",
    "    ),\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "# Create a transformer for the machine translation. It will be slightly larger than the simple sequence\n",
    "# reverser, but still extremely small compared to LLMs\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "d_ff = 512\n",
    "max_seq_length = 256\n",
    "dropout = 0.1\n",
    "source_vocabsize = len(de_vocab) # approximately 30k\n",
    "target_vocabsize = len(en_vocab) # approximately 30k\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the transforer\n",
    "ger_eng_translator = Transformer(source_vocabsize,target_vocabsize,\n",
    "                          d_model,num_heads,num_layers,d_ff,max_seq_length,dropout)\n",
    "\n",
    "ger_eng_translator.to(device)\n",
    "# Instantiate the optimizer\n",
    "optim = torch.optim.AdamW(ger_eng_translator.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "steps_per_epoch = len(train_loader)\n",
    "# Train the transformer for 60 epochs\n",
    "num_epochs = 60\n",
    "for e in range(num_epochs):\n",
    "    running_train_loss = 0\n",
    "    i = 0 # number of training iterations per epoch\n",
    "    for src, tgt in train_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        # Previous outputs are fed back as input into the decoder, and the transformer keeps predicting the \n",
    "        # next output in the sequence until the EOS is reached.\n",
    "        # tgt_in is fed into the decoder and tgt_out is compared against the actual transformer outputs\n",
    "        tgt_in, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
    "        # Forward propagation\n",
    "        logits = ger_eng_translator(src, tgt_in, pad_idx)\n",
    "        # Loss calculation\n",
    "        loss = nn.functional.cross_entropy(logits.permute(0,2,1), tgt_out, ignore_index=pad_idx)\n",
    "\n",
    "        # Backward propagation\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(ger_eng_translator.parameters(), 1.)\n",
    "        # Update model parameters\n",
    "        optim.step()\n",
    "        \n",
    "        i=i+1 \n",
    "        print (f\"Completed training step: {i}/{steps_per_epoch} in epoch: {e+1}.Training loss: {loss}\",end='\\r')\n",
    "        running_train_loss+=loss.item()\n",
    "    print(\"Training loss : \",running_train_loss/steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German : \n",
      "ein Frau in schwarzer Jacke, der auf einem weißen Pferd reitet.\n",
      "\n",
      "English : \n",
      "a woman in a black jacket riding a white horse .\n"
     ]
    }
   ],
   "source": [
    "sentence_1 = 'zwei frauen spazieren und lachen im park.'\n",
    "sentence_2 = 'ein Mann, der mit seinem Hund spazieren geht.'\n",
    "sentence_3 = 'ein Frau in schwarzer Jacke, der auf einem weißen Pferd reitet.'\n",
    "sentence_4 = 'Fünf Wanderer, von denen einer in Richtung Kamera blickt und die anderen von der Kamera weg, gehen durch ein steiniges Flussbett.'\n",
    "sentence_5 = 'Ein Mann isst eine Orange, während er mit seinem Sohn spricht.'\n",
    "\n",
    "testSentence = sentence_3\n",
    "\n",
    "x = torch.LongTensor(de_vocab(tokenizer(testSentence))).unsqueeze(dim=0).to(device)\n",
    "y = ger_eng_translator.do_inference(x, None, en_vocab[BOS], en_vocab[EOS])\n",
    "\n",
    "# Print the German sentence\n",
    "print(\"German : \")\n",
    "print(testSentence)\n",
    "\n",
    "print()\n",
    "\n",
    "# Print the English sentence, but remove the BOS & EOS tokens\n",
    "print(\"English : \")\n",
    "print(' '.join(en_vocab.lookup_tokens(y[0].tolist())[1:-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
